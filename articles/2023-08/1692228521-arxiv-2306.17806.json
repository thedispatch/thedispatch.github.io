{
  "uuid": "cc19b2db-9620-4ec2-9484-02053395b438",
  "version": "0.1",
  "sources": [
    {
      "id": "2306.17806",
      "url": "https://arxiv.org/abs/2306.17806",
      "site": "arxiv"
    }
  ],
  "components": [
    {
      "source_url": "https://arxiv.org/abs/2306.17806",
      "type": "abstract",
      "content": "Stay on topic with Classifier-Free Guidance\n\nAuthors:Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi, Stella Biderman\n\nClassifier-Free Guidance (CFG) has recently emerged in text-to-image\ngeneration as a lightweight technique to encourage prompt-adherence in\ngenerations. In this work, we demonstrate that CFG can be used broadly as an\ninference-time technique in pure language modeling. We show that CFG (1)\nimproves the performance of Pythia, GPT-2 and LLaMA-family models across an\narray of tasks: Q\\&A, reasoning, code generation, and machine translation,\nachieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements\nequivalent to a model with twice the parameter-count; (3) can stack alongside\nother inference-time methods like Chain-of-Thought and Self-Consistency,\nyielding further improvements in difficult tasks; (4) can be used to increase\nthe faithfulness and coherence of assistants in challenging form-driven and\ncontent-driven prompts: in a human evaluation we show a 75\\% preference for\nGPT4All using CFG over baseline.\n\n                            Skip to main content       We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate        >  cs  > arXiv:2306.17806       Help | Advanced Search      All fields  Title  Author  Abstract  Comments  Journal reference  ACM classification  MSC classification  Report number  arXiv identifier  DOI  ORCID  arXiv author ID  Help pages  Full text      Search                        GO        quick links   Login  Help Pages  About               Computer Science > Computation and Language    arXiv:2306.17806 (cs)     [Submitted on 30 Jun 2023]  Title: Stay on topic with Classifier-Free Guidance  Authors: Guillaume Sanchez , Honglu Fan , Alexander Spangher , Elad Levi , Pawan Sasanka Ammanamanchi , Stella Biderman  Download a PDF of the paper titled Stay on topic with Classifier-Free Guidance, by Guillaume Sanchez and 5 other authors  Download PDF   Abstract: Classifier-Free Guidance (CFG) has recently emerged in text-to-image\ngeneration as a lightweight technique to encourage prompt-adherence in\ngenerations. In this work, we demonstrate that CFG can be used broadly as an\ninference-time technique in pure language modeling. We show that CFG (1)\nimproves the performance of Pythia, GPT-2 and LLaMA-family models across an\narray of tasks: Q\\&A, reasoning, code generation, and machine translation,\nachieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements\nequivalent to a model with twice the parameter-count; (3) can stack alongside\nother inference-time methods like Chain-of-Thought and Self-Consistency,\nyielding further improvements in difficult tasks; (4) can be used to increase\nthe faithfulness and coherence of assistants in challenging form-driven and\ncontent-driven prompts: in a human evaluation we show a 75\\% preference for\nGPT4All using CFG over baseline.     Subjects:   Computation and Language (cs.CL) ; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)   Cite as:  arXiv:2306.17806 [cs.CL]      (or  arXiv:2306.17806v1 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.2306.17806     Focus to learn more     arXiv-issued DOI via DataCite         Submission history From: Guillaume Sanchez [ view email ]  [v1] Fri, 30 Jun 2023 17:07:02 UTC (3,096 KB)        Full-text links:  Download:   Download a PDF of the paper titled Stay on topic with Classifier-Free Guidance, by Guillaume Sanchez and 5 other authors  PDF  Other formats     Current browse context: cs.CL    <\u00a0prev   |   next\u00a0>    new  |  recent  |  2306  Change to browse by:  cs  cs.CV  cs.LG      References & Citations   NASA ADS Google Scholar  Semantic Scholar      a  export BibTeX citation  Loading...      BibTeX formatted citation  \u00d7    loading...    Data provided by:       Bookmark             Bibliographic Tools   Bibliographic and Citation Tools        Bibliographic Explorer Toggle     Bibliographic Explorer  ( What is the Explorer? )         Litmaps Toggle     Litmaps  ( What is Litmaps? )         scite.ai Toggle     scite Smart Citations  ( What are Smart Citations? )          Code, Data, Media   Code, Data and Media Associated with this Article        Links to Code Toggle     CatalyzeX Code Finder for Papers  ( What is CatalyzeX? )         DagsHub Toggle     DagsHub  ( What is DagsHub? )         Links to Code Toggle     Papers with Code  ( What is Papers with Code? )         ScienceCast Toggle     ScienceCast  ( What is ScienceCast? )            Demos   Demos        Replicate Toggle     Replicate  ( What is Replicate? )         Spaces Toggle     Hugging Face Spaces  ( What is Spaces? )         Related Papers   Recommenders and Search Tools        Link to Influence Flower     Influence Flower  ( What are Influence Flowers? )         Connected Papers Toggle     Connected Papers  ( What is Connected Papers? )         Core recommender toggle     CORE Recommender  ( What is CORE? )       Author  Venue  Institution  Topic               About arXivLabs     arXivLabs: experimental projects with community collaborators  arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.  Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.  Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs .            Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )              About  Help       Click here to contact arXiv  Contact    Click here to subscribe  Subscribe             Copyright  Privacy Policy      Web Accessibility Assistance    arXiv Operational Status Get status notifications via email or slack              "
    },
    {
      "source_url": "https://arxiv.org/abs/2306.17806",
      "type": "generated",
      "content": "- The paper is titled \"Stay on topic with Classifier-Free Guidance\" and is authored by Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi, and Stella Biderman.\n- The paper introduces Classifier-Free Guidance (CFG) as a technique to encourage prompt-adherence in text-to-image generation and demonstrates its broader applicability in pure language modeling.\n- The authors show that CFG improves the performance of Pythia, GPT-2, and LLaMA-family models across various tasks such as Q&A, reasoning, code generation, and machine translation.\n- CFG achieves state-of-the-art results on the LAMBADA dataset with LLaMA-7B over PaLM-540B.\n- The improvements brought by CFG are equivalent to those of a model with twice the parameter-count.\n- CFG can be combined with other inference-time methods like Chain-of-Thought and Self-Consistency, leading to further improvements in challenging tasks.\n- CFG can be used to enhance the faithfulness and coherence of assistants in form-driven and content-driven prompts, as demonstrated by a human evaluation where GPT4All using CFG was preferred 75% of the time over the baseline."
    }
  ],
  "filename": "1692228521-arxiv-2306.17806"
}