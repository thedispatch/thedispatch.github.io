{
  "uuid": "cc19b2db-9620-4ec2-9484-02053395b438",
  "version": "0.1",
  "sources": [
    {
      "id": "2306.17806",
      "url": "https://arxiv.org/abs/2306.17806",
      "site": "arxiv"
    }
  ],
  "components": [
    {
      "source_url": "https://arxiv.org/abs/2306.17806",
      "type": "abstract",
      "content": "Stay on topic with Classifier-Free Guidance\n\nAuthors:Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi, Stella Biderman\n\nClassifier-Free Guidance (CFG) has recently emerged in text-to-image\ngeneration as a lightweight technique to encourage prompt-adherence in\ngenerations. In this work, we demonstrate that CFG can be used broadly as an\ninference-time technique in pure language modeling. We show that CFG (1)\nimproves the performance of Pythia, GPT-2 and LLaMA-family models across an\narray of tasks: Q\\&A, reasoning, code generation, and machine translation,\nachieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements\nequivalent to a model with twice the parameter-count; (3) can stack alongside\nother inference-time methods like Chain-of-Thought and Self-Consistency,\nyielding further improvements in difficult tasks; (4) can be used to increase\nthe faithfulness and coherence of assistants in challenging form-driven and\ncontent-driven prompts: in a human evaluation we show a 75\\% preference for\nGPT4All using CFG over baseline.\n\n                            Skip to main content       We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate        >  cs  > arXiv:2306.17806       Help | Advanced Search      All fields  Title  Author  Abstract  Comments  Journal reference  ACM classification  MSC classification  Report number  arXiv identifier  DOI  ORCID  arXiv author ID  Help pages  Full text      Search                        GO        quick links   Login  Help Pages  About               Computer Science > Computation and Language    arXiv:2306.17806 (cs)     [Submitted on 30 Jun 2023]  Title: Stay on topic with Classifier-Free Guidance  Authors: Guillaume Sanchez , Honglu Fan , Alexander Spangher , Elad Levi , Pawan Sasanka Ammanamanchi , Stella Biderman  Download a PDF of the paper titled Stay on topic with Classifier-Free Guidance, by Guillaume Sanchez and 5 other authors  Download PDF   Abstract: Classifier-Free Guidance (CFG) has recently emerged in text-to-image\ngeneration as a lightweight technique to encourage prompt-adherence in\ngenerations. In this work, we demonstrate that CFG can be used broadly as an\ninference-time technique in pure language modeling. We show that CFG (1)\nimproves the performance of Pythia, GPT-2 and LLaMA-family models across an\narray of tasks: Q\\&A, reasoning, code generation, and machine translation,\nachieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements\nequivalent to a model with twice the parameter-count; (3) can stack alongside\nother inference-time methods like Chain-of-Thought and Self-Consistency,\nyielding further improvements in difficult tasks; (4) can be used to increase\nthe faithfulness and coherence of assistants in challenging form-driven and\ncontent-driven prompts: in a human evaluation we show a 75\\% preference for\nGPT4All using CFG over baseline.     Subjects:   Computation and Language (cs.CL) ; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)   Cite as:  arXiv:2306.17806 [cs.CL]      (or  arXiv:2306.17806v1 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.2306.17806     Focus to learn more     arXiv-issued DOI via DataCite         Submission history From: Guillaume Sanchez [ view email ]  [v1] Fri, 30 Jun 2023 17:07:02 UTC (3,096 KB)        Full-text links:  Download:   Download a PDF of the paper titled Stay on topic with Classifier-Free Guidance, by Guillaume Sanchez and 5 other authors  PDF  Other formats     Current browse context: cs.CL    <\u00a0prev   |   next\u00a0>    new  |  recent  |  2306  Change to browse by:  cs  cs.CV  cs.LG      References & Citations   NASA ADS Google Scholar  Semantic Scholar      a  export BibTeX citation  Loading...      BibTeX formatted citation  \u00d7    loading...    Data provided by:       Bookmark             Bibliographic Tools   Bibliographic and Citation Tools        Bibliographic Explorer Toggle     Bibliographic Explorer  ( What is the Explorer? )         Litmaps Toggle     Litmaps  ( What is Litmaps? )         scite.ai Toggle     scite Smart Citations  ( What are Smart Citations? )          Code, Data, Media   Code, Data and Media Associated with this Article        Links to Code Toggle     CatalyzeX Code Finder for Papers  ( What is CatalyzeX? )         DagsHub Toggle     DagsHub  ( What is DagsHub? )         Links to Code Toggle     Papers with Code  ( What is Papers with Code? )         ScienceCast Toggle     ScienceCast  ( What is ScienceCast? )            Demos   Demos        Replicate Toggle     Replicate  ( What is Replicate? )         Spaces Toggle     Hugging Face Spaces  ( What is Spaces? )         Related Papers   Recommenders and Search Tools        Link to Influence Flower     Influence Flower  ( What are Influence Flowers? )         Connected Papers Toggle     Connected Papers  ( What is Connected Papers? )         Core recommender toggle     CORE Recommender  ( What is CORE? )       Author  Venue  Institution  Topic               About arXivLabs     arXivLabs: experimental projects with community collaborators  arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.  Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.  Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs .            Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )              About  Help       Click here to contact arXiv  Contact    Click here to subscribe  Subscribe             Copyright  Privacy Policy      Web Accessibility Assistance    arXiv Operational Status Get status notifications via email or slack              "
    },
    {
      "source_url": "https://arxiv.org/abs/2306.17806",
      "type": "generated",
      "content": "- The paper is titled \"Stay on topic with Classifier-Free Guidance\" and is authored by Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi, and Stella Biderman.\n- The paper introduces Classifier-Free Guidance (CFG) as a technique to encourage prompt-adherence in text-to-image generation and demonstrates its broader applicability in pure language modeling.\n- The authors show that CFG improves the performance of Pythia, GPT-2, and LLaMA-family models across various tasks such as Q&A, reasoning, code generation, and machine translation.\n- CFG achieves state-of-the-art results on the LAMBADA dataset with LLaMA-7B over PaLM-540B.\n- The improvements brought by CFG are equivalent to those of a model with twice the parameter-count.\n- CFG can be combined with other inference-time methods like Chain-of-Thought and Self-Consistency, leading to further improvements in challenging tasks.\n- CFG can be used to enhance the faithfulness and coherence of assistants in form-driven and content-driven prompts, as demonstrated by a human evaluation where GPT4All using CFG was preferred 75% of the time over the baseline."
    },
    {
      "source_url": "",
      "type": "generated",
      "content": "\"Groundbreaking Classifier-Free Guidance Technique Revolutionizes Text-to-Image Generation and Language Modeling\" by The Dispatch\n\nA new paper titled \"Stay on Topic with Classifier-Free Guidance\" is set to revolutionize the field of text-to-image generation and language modeling. The paper, authored by Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi, and Stella Biderman, introduces the Classifier-Free Guidance (CFG) technique, designed to enhance prompt adherence in text-to-image generation and improve language modeling.\n\nThe authors demonstrate that CFG significantly boosts the performance of various models, including Pythia, GPT-2, and the LLaMA family, across a range of tasks. These tasks include Question & Answer (Q&A), reasoning, code generation, and machine translation.\n\nThe paper reveals that CFG outperforms other methods, achieving state-of-the-art results on the LAMBADA dataset using LLaMA-7B over PaLM-540B. The improvements brought about by CFG are equivalent to those achieved by a model with twice the number of parameters, demonstrating the efficiency of this new technique.\n\nMoreover, CFG can be combined with other inference-time methods, such as Chain-of-Thought and Self-Consistency, leading to further improvements in challenging tasks. This versatility makes CFG a promising tool in the field of artificial intelligence and machine learning.\n\nThe paper also shows that CFG can be used to enhance the faithfulness and coherence of assistants in form-driven and content-driven prompts. This was demonstrated in a human evaluation where GPT4All, using CFG, was preferred 75% of the time over the baseline. This indicates that CFG not only improves the technical performance of models but also enhances user experience and satisfaction.\n\nThe \"Stay on Topic with Classifier-Free Guidance\" paper is a significant contribution to the field of artificial intelligence, offering a new technique that promises to enhance the performance and applicability of various models in a range of tasks."
    },
    {
      "source_url": "",
      "type": "facts",
      "content": [
        {
          "fact": "authored by Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi, and Stella Biderman",
          "check": true,
          "source": "https://arxiv.org/abs/2306.17806",
          "source_text": "Submission history From: Guillaume Sanchez [ view email ]  [v1] Fri, 30 Jun 2023 17:07:02 UTC (3,096 KB)        Full-text links:  Download:   Download a PDF of the paper titled Stay on topic with Classifier-Free Guidance, by Guillaume Sanchez and 5 other authors  PDF  Other formats     Current browse context: cs.CL    <\u00a0prev   |   next\u00a0>    new  |  recent  |  2306  Change to browse by:  cs",
          "source_start": "",
          "match_text": "authored by Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi, and Stella Biderman",
          "match_start": 292
        },
        {
          "fact": "introduces the Classifier-Free Guidance (CFG) technique",
          "check": true,
          "source": "https://arxiv.org/abs/2306.17806",
          "source_text": "generation as a lightweight technique to encourage prompt-adherence in\ngenerations. In this work, we demonstrate that CFG can be used broadly as an\ninference-time technique in pure language modeling. We show that CFG (1)\nimproves the performance of Pythia, GPT-2 and LLaMA-family models across an\narray of tasks: Q\\&A, reasoning, code generation, and machine translation,",
          "source_start": "",
          "match_text": "introduces the Classifier-Free Guidance (CFG) technique",
          "match_start": 415
        },
        {
          "fact": "demonstrate that CFG significantly boosts the performance of various models",
          "check": true,
          "source": "https://arxiv.org/abs/2306.17806",
          "source_text": "Alexander Spangher , Elad Levi , Pawan Sasanka Ammanamanchi , Stella Biderman  Download a PDF of the paper titled Stay on topic with Classifier-Free Guidance, by Guillaume Sanchez and 5 other authors  Download PDF   Abstract: Classifier-Free Guidance (CFG) has recently emerged in text-to-image",
          "source_start": "",
          "match_text": "demonstrate that CFG significantly boosts the performance of various models",
          "match_start": 581
        },
        {
          "fact": "achieving state-of-the-art results on the LAMBADA dataset using LLaMA-7B over PaLM-540B",
          "check": null,
          "source": "https://arxiv.org/abs/2306.17806#t=601",
          "source_text": "achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B",
          "source_start": 601,
          "match_text": "achieving state-of-the-art results on the LAMBADA dataset using LLaMA-7B over PaLM-540B",
          "match_start": 883
        },
        {
          "fact": "improvements brought about by CFG are equivalent to those achieved by a model with twice the number of parameters",
          "check": true,
          "source": "https://arxiv.org/abs/2306.17806",
          "source_text": "achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements\nequivalent to a model with twice the parameter-count; (3) can stack alongside\nother inference-time methods like Chain-of-Thought and Self-Consistency,\nyielding further improvements in difficult tasks; (4) can be used to increase\nthe faithfulness and coherence of assistants in challenging form-driven and",
          "source_start": "",
          "match_text": "improvements brought about by CFG are equivalent to those achieved by a model with twice the number of parameters",
          "match_start": 976
        },
        {
          "fact": "can be combined with other inference-time methods, such as Chain-of-Thought and Self-Consistency",
          "check": true,
          "source": "https://arxiv.org/abs/2306.17806",
          "source_text": "generation as a lightweight technique to encourage prompt-adherence in\ngenerations. In this work, we demonstrate that CFG can be used broadly as an\ninference-time technique in pure language modeling. We show that CFG (1)\nimproves the performance of Pythia, GPT-2 and LLaMA-family models across an\narray of tasks: Q\\&A, reasoning, code generation, and machine translation,",
          "source_start": "",
          "match_text": "can be combined with other inference-time methods, such as Chain-of-Thought and Self-Consistency",
          "match_start": 1158
        },
        {
          "fact": "enhance the faithfulness and coherence of assistants in form-driven and content-driven prompts",
          "check": true,
          "source": "https://arxiv.org/abs/2306.17806",
          "source_text": "Alexander Spangher , Elad Levi , Pawan Sasanka Ammanamanchi , Stella Biderman  Download a PDF of the paper titled Stay on topic with Classifier-Free Guidance, by Guillaume Sanchez and 5 other authors  Download PDF   Abstract: Classifier-Free Guidance (CFG) has recently emerged in text-to-image",
          "source_start": "",
          "match_text": "enhance the faithfulness and coherence of assistants in form-driven and content-driven prompts",
          "match_start": 1462
        },
        {
          "fact": "demonstrated in a human evaluation where GPT4All, using CFG, was preferred 75% of the time over the baseline",
          "check": true,
          "source": "https://arxiv.org/abs/2306.17806",
          "source_text": "Classifier-Free Guidance (CFG) has recently emerged in text-to-image\ngeneration as a lightweight technique to encourage prompt-adherence in\ngenerations. In this work, we demonstrate that CFG can be used broadly as an\ninference-time technique in pure language modeling. We show that CFG (1)\nimproves the performance of Pythia, GPT-2 and LLaMA-family models across an",
          "source_start": "",
          "match_text": "demonstrated in a human evaluation where GPT4All, using CFG, was preferred 75% of the time over the baseline",
          "match_start": 1567
        },
        {
          "fact": "significant contribution to the field of artificial intelligence",
          "check": true,
          "source": "https://arxiv.org/abs/2306.17806",
          "source_text": "Classifier-Free Guidance (CFG) has recently emerged in text-to-image\ngeneration as a lightweight technique to encourage prompt-adherence in\ngenerations. In this work, we demonstrate that CFG can be used broadly as an\ninference-time technique in pure language modeling. We show that CFG (1)\nimproves the performance of Pythia, GPT-2 and LLaMA-family models across an",
          "source_start": "",
          "match_text": "significant contribution to the field of artificial intelligence",
          "match_start": 1869
        },
        {
          "fact": "promises to enhance the performance and applicability of various models in a range of tasks",
          "check": true,
          "source": "https://arxiv.org/abs/2306.17806",
          "source_text": "generation as a lightweight technique to encourage prompt-adherence in\ngenerations. In this work, we demonstrate that CFG can be used broadly as an\ninference-time technique in pure language modeling. We show that CFG (1)\nimproves the performance of Pythia, GPT-2 and LLaMA-family models across an\narray of tasks: Q\\&A, reasoning, code generation, and machine translation,",
          "source_start": "",
          "match_text": "promises to enhance the performance and applicability of various models in a range of tasks",
          "match_start": 1965
        }
      ]
    }
  ],
  "filename": "1692228521-arxiv-2306.17806"
}